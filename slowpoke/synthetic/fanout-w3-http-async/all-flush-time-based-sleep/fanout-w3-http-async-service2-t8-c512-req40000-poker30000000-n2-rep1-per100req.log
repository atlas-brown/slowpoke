[config.py] Random numbers for execution time: [467.7110783366884, 525.184656690112, 990.1754468108787, 1144.090129435625]
benchmark                        : synthetic
num_threads                      : 8
num_conns                        : 512
target_service                   : service2
request_type                     : fanout-w3-http-async
repetitions                      : 1
target_num_exp                   : 2
pre_run                          : False
num_req                          : 40000
groundtruths                     : []
slowdowns                        : []
predicteds                       : []
errs                             : []
client_cpu_quota                 : 2
random_seed                      : 22197
request_ratio                    : {'service0': 1, 'service1': 1, 'service2': 1, 'service3': 1}
baseline_service_processing_time : {'service0': 1144.09, 'service1': 990.18, 'service2': 1050.37, 'service3': 467.71}
cpu_quota                        : {'service0': 2, 'service1': 2, 'service2': 2, 'service3': 2}
target_processing_time_range     : [0, 1050.37]
baseline_throughputs             : []
poker_batch                      : 30000000
[test.py] Starting experiment...
[test.py] >>>>>>>>>>>>>>>>>>>>>>>>>>>>> Running experiment 0...
[test.py] Actual processing time range: [0, 525]
[test.py] Running baseline experiment
    [exp] Running (pre_run: False) workload synthetic/fanout-w3-http-async request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '30000000', 'PROCESSING_TIME_SERVICE0': '0.00114409', 'PROCESSING_TIME_SERVICE1': '0.00099018', 'PROCESSING_TIME_SERVICE2': '0.00105037', 'PROCESSING_TIME_SERVICE3': '0.00046771', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE3': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE3': '10'}
    [exp] Executing cmd `bash run.sh synthetic fanout-w3-http-async 8 512 40000`:
        [run.sh] Running benchmark synthetic with request fanout-w3-http-async, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        configmap "config-service3" deleted
        deployment.apps "service3" deleted
        service "service3" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        configmap/config-service3 created
        deployment.apps/service3 created
        service/service3 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   524.97ms  268.51ms   1.93s    85.55%
        Req/Sec   122.50     45.81   262.00     70.98%
        Latency Distribution
        50%  454.51ms
        75%  525.32ms
        90%  907.28ms
        99%    1.66s
        2767 requests in 3.03s, 1.82MB read
        Socket errors: connect 0, read 0, write 0, timeout 1
        Requests/sec:    911.95
        Transfer/sec:    613.61KB
        [run.sh] Speed is 911.95, duration is 87
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d87s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-65b8cdd65c-bb2cx        1866m        62Mi
        service1-8497d67d4b-xt4kb        1234m        14Mi
        service2-6d44f9d775-lqtmg        1265m        16Mi
        service3-6c849b8bcc-9wgb6        651m         12Mi
        ubuntu-client-76886f6bbd-rpjsz   45m          13Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   489.44ms   84.78ms   1.37s    85.10%
        Req/Sec   131.18     38.14   353.00     65.35%
        Latency Distribution
        50%  476.32ms
        75%  516.79ms
        90%  569.54ms
        99%  829.02ms
        40000 requests in 1.45m, 26.28MB read
        Requests/sec:    459.77
        Transfer/sec:    309.36KB
        ------------------------------
        stop time: 38.043018
        stop time: 38.280349
        stop time: 38.317936
        stop time: 38.536081
        stop time: 38.596363
        stop time: 38.673299
        stop time: 38.614142
        stop time: 38.686182
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [38.043018, 38.280349, 38.317936, 38.536081, 38.596363, 38.673299, 38.614142, 38.686182]
    [exp] Throughput: 1039.813922698998
[test.py] Baseline throughput: 1039.813922698998
[test.py] Running groundtruth experiment
    [exp] Running (pre_run: False) workload synthetic/fanout-w3-http-async request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '30000000', 'PROCESSING_TIME_SERVICE0': '0.00114409', 'PROCESSING_TIME_SERVICE1': '0.00099018', 'PROCESSING_TIME_SERVICE2': '0.0', 'PROCESSING_TIME_SERVICE3': '0.00046771', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE3': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE3': '10'}
    [exp] Executing cmd `bash run.sh synthetic fanout-w3-http-async 8 512 40000`:
        [run.sh] Running benchmark synthetic with request fanout-w3-http-async, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        configmap "config-service3" deleted
        deployment.apps "service3" deleted
        service "service3" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        configmap/config-service3 created
        deployment.apps/service3 created
        service/service3 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   517.26ms  210.33ms   1.57s    80.93%
        Req/Sec   121.03     56.49   270.00     64.68%
        Latency Distribution
        50%  459.39ms
        75%  584.09ms
        90%  789.68ms
        99%    1.35s
        2678 requests in 3.03s, 1.76MB read
        Requests/sec:    884.90
        Transfer/sec:    595.40KB
        [run.sh] Speed is 884.90, duration is 90
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d90s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                        CPU(cores)   MEMORY(bytes)
        service0-65b8cdd65c-jkr5c   139m         64Mi
        service1-8497d67d4b-wd66n   160m         12Mi
        Running 2m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   457.78ms  103.43ms   1.33s    76.72%
        Req/Sec   139.97     52.24   320.00     70.27%
        Latency Distribution
        50%  441.62ms
        75%  503.35ms
        90%  581.05ms
        99%  812.27ms
        40000 requests in 1.50m, 26.28MB read
        Requests/sec:    444.44
        Transfer/sec:    299.04KB
        ------------------------------
        stop time: 35.797563
        stop time: 35.974535
        stop time: 36.049308
        stop time: 35.749439
        stop time: 36.045705
        stop time: 36.145186
        stop time: 36.109982
        stop time: 36.121017
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [35.797563, 35.974535, 36.049308, 35.749439, 36.045705, 36.145186, 36.109982, 36.121017]
    [exp] Throughput: 1111.1391403675514
    [exp] Running (pre_run: False) workload synthetic/fanout-w3-http-async request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '30000000', 'PROCESSING_TIME_SERVICE0': '0.00114409', 'PROCESSING_TIME_SERVICE1': '0.00099018', 'PROCESSING_TIME_SERVICE2': '0.000525', 'PROCESSING_TIME_SERVICE3': '0.00046771', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE3': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE3': '10'}
    [exp] Executing cmd `bash run.sh synthetic fanout-w3-http-async 8 512 40000`:
        [run.sh] Running benchmark synthetic with request fanout-w3-http-async, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        configmap "config-service3" deleted
        deployment.apps "service3" deleted
        service "service3" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        configmap/config-service3 created
        deployment.apps/service3 created
        service/service3 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   502.87ms  346.67ms   2.00s    82.21%
        Req/Sec   125.44     49.84   242.00     68.53%
        Latency Distribution
        50%  419.87ms
        75%  566.13ms
        90%  959.02ms
        99%    1.84s
        2921 requests in 3.03s, 1.92MB read
        Socket errors: connect 0, read 0, write 0, timeout 28
        Requests/sec:    965.35
        Transfer/sec:    649.98KB
        [run.sh] Speed is 965.35, duration is 82
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d82s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-65b8cdd65c-h2ntk        1926m        74Mi
        service1-8497d67d4b-wwlv9        1252m        15Mi
        service2-6d44f9d775-4hllv        752m         14Mi
        service3-6c849b8bcc-xb5lq        681m         14Mi
        ubuntu-client-76886f6bbd-x485p   57m          15Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   458.44ms  130.48ms   1.47s    80.09%
        Req/Sec   139.87     51.21   360.00     71.46%
        Latency Distribution
        50%  435.30ms
        75%  504.60ms
        90%  601.84ms
        99%  930.61ms
        40000 requests in 1.37m, 26.28MB read
        Requests/sec:    487.80
        Transfer/sec:    328.22KB
        ------------------------------
        stop time: 35.573115
        stop time: 36.002590
        stop time: 35.866367
        stop time: 36.023996
        stop time: 36.174290
        stop time: 36.250241
        stop time: 36.135177
        stop time: 36.147386
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [35.573115, 36.00259, 35.866367, 36.023996, 36.17429, 36.250241, 36.135177, 36.147386]
    [exp] Throughput: 1110.4434492758212
[test.py] Running slowdown experiment
    [exp] Running (pre_run: False) workload synthetic/fanout-w3-http-async request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '30000000', 'PROCESSING_TIME_SERVICE0': '0.00114409', 'PROCESSING_TIME_SERVICE1': '0.00099018', 'PROCESSING_TIME_SERVICE2': '0.00105037', 'PROCESSING_TIME_SERVICE3': '0.00046771', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '525.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '52500000', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '525.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '52500000', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE3': '525.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE3': '52500000'}
    [exp] Executing cmd `bash run.sh synthetic fanout-w3-http-async 8 512 40000`:
        [run.sh] Running benchmark synthetic with request fanout-w3-http-async, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        configmap "config-service3" deleted
        deployment.apps "service3" deleted
        service "service3" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        configmap/config-service3 created
        deployment.apps/service3 created
        service/service3 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   722.16ms  191.12ms   1.75s    73.22%
        Req/Sec    88.42     48.63   220.00     63.00%
        Latency Distribution
        50%  694.96ms
        75%  820.75ms
        90%    1.04s
        99%    1.25s
        1867 requests in 3.03s, 1.23MB read
        Requests/sec:    615.24
        Transfer/sec:    414.18KB
        [run.sh] Speed is 615.24, duration is 130
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d130s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-6cc9884954-4g22c        501m         69Mi
        service1-59f694ff57-hhsll        368m         15Mi
        service2-6d44f9d775-qjnbd        238m         14Mi
        service3-5fd4b4dbfd-g578s        105m         15Mi
        ubuntu-client-76886f6bbd-vl9sw   5m           13Mi
        Running 2m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   721.10ms  152.31ms   2.17s    79.72%
        Req/Sec    89.71     43.67   292.00     66.61%
        Latency Distribution
        50%  706.37ms
        75%  773.63ms
        90%  877.58ms
        99%    1.27s
        40000 requests in 2.17m, 26.29MB read
        Requests/sec:    307.69
        Transfer/sec:    207.09KB
        ------------------------------
        stop time: 56.470435
        stop time: 56.340593
        stop time: 56.624841
        stop time: 56.736432
        stop time: 56.717965
        stop time: 56.716691
        stop time: 56.940796
        stop time: 57.002594
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [56.470435, 56.340593, 56.624841, 56.736432, 56.717965, 56.716691, 56.940796, 57.002594]
    [exp] Throughput: 705.544604070163
    [exp] Running (pre_run: False) workload synthetic/fanout-w3-http-async request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '30000000', 'PROCESSING_TIME_SERVICE0': '0.00114409', 'PROCESSING_TIME_SERVICE1': '0.00099018', 'PROCESSING_TIME_SERVICE2': '0.00105037', 'PROCESSING_TIME_SERVICE3': '0.00046771', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '262.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '26250000', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '262.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '26250000', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE3': '262.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE3': '26250000'}
    [exp] Executing cmd `bash run.sh synthetic fanout-w3-http-async 8 512 40000`:
        [run.sh] Running benchmark synthetic with request fanout-w3-http-async, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        configmap "config-service3" deleted
        deployment.apps "service3" deleted
        service "service3" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        configmap/config-service3 created
        deployment.apps/service3 created
        service/service3 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   625.14ms  236.29ms   1.83s    74.27%
        Req/Sec    99.49     47.96   242.00     63.01%
        Latency Distribution
        50%  556.35ms
        75%  733.41ms
        90%  977.02ms
        99%    1.43s
        2221 requests in 3.03s, 1.47MB read
        Requests/sec:    734.14
        Transfer/sec:    496.62KB
        [run.sh] Speed is 734.14, duration is 108
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d108s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-59bc48f69b-52qqc        323m         65Mi
        service1-64bcdd59f-vrh8g         234m         13Mi
        service2-6d44f9d775-nls9k        334m         12Mi
        service3-7d7664b588-qxs88        173m         11Mi
        ubuntu-client-76886f6bbd-gqz28   19m          0Mi
        Running 2m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   586.99ms  151.30ms   2.02s    79.44%
        Req/Sec   109.15     46.08   313.00     68.70%
        Latency Distribution
        50%  566.79ms
        75%  643.78ms
        90%  750.88ms
        99%    1.16s
        40000 requests in 1.80m, 26.30MB read
        Requests/sec:    370.37
        Transfer/sec:    249.33KB
        ------------------------------
        stop time: 46.129259
        stop time: 45.964417
        stop time: 46.074938
        stop time: 45.959482
        stop time: 46.062024
        stop time: 46.446505
        stop time: 46.295818
        stop time: 46.402359
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [46.129259, 45.964417, 46.074938, 45.959482, 46.062024, 46.446505, 46.295818, 46.402359]
    [exp] Throughput: 866.4225474208088
[test.py] Baseline throughput:  1039.813922698998
[test.py] Groundtruth:  [1111.1391403675514, 1110.4434492758212]
[test.py] Slowdown:  [705.544604070163, 866.4225474208088]
[test.py] Predicted:  [1120.8753874248857, 1121.7222845436322]
[test.py] Error percentage:  [0.8762401308366956, 1.015705507125693]
[test.py] ++++++++++++++++++++++++++++++++ Summary: 
[test.py] Result for the experiment 0: 
    Baseline throughput: 1039.813922698998
    Groundtruth: [1111.1391403675514, 1110.4434492758212]
    Slowdown:    [705.544604070163, 866.4225474208088]
    Predicted:   [1120.8753874248857, 1121.7222845436322]
    Error Perc:  [0.8762401308366956, 1.015705507125693]
