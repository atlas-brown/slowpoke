[config.py] Random numbers for execution time: [266.13634741120245, 341.46867326669883, 1140.7815626331162, 1275.9127585791719]
benchmark                        : synthetic
num_threads                      : 8
num_conns                        : 512
target_service                   : service0
request_type                     : fanout-w3-http-async
repetitions                      : 1
target_num_exp                   : 2
pre_run                          : False
num_req                          : 40000
groundtruths                     : []
slowdowns                        : []
predicteds                       : []
errs                             : []
client_cpu_quota                 : 2
random_seed                      : 24479
request_ratio                    : {'service0': 1, 'service1': 1, 'service2': 1, 'service3': 1}
baseline_service_processing_time : {'service0': 1275.91, 'service1': 1140.78, 'service2': 341.47, 'service3': 266.14}
cpu_quota                        : {'service0': 2, 'service1': 2, 'service2': 2, 'service3': 2}
target_processing_time_range     : [0, 1275.91]
baseline_throughputs             : []
poker_batch                      : 30000000
[test.py] Starting experiment...
[test.py] >>>>>>>>>>>>>>>>>>>>>>>>>>>>> Running experiment 0...
[test.py] Actual processing time range: [0, 637]
[test.py] Running baseline experiment
    [exp] Running (pre_run: False) workload synthetic/fanout-w3-http-async request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '30000000', 'PROCESSING_TIME_SERVICE0': '0.00127591', 'PROCESSING_TIME_SERVICE1': '0.00114078', 'PROCESSING_TIME_SERVICE2': '0.00034147', 'PROCESSING_TIME_SERVICE3': '0.00026614', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE3': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE3': '10'}
    [exp] Executing cmd `bash run.sh synthetic fanout-w3-http-async 8 512 40000`:
        [run.sh] Running benchmark synthetic with request fanout-w3-http-async, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        configmap "config-service3" deleted
        deployment.apps "service3" deleted
        service "service3" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        configmap/config-service3 created
        deployment.apps/service3 created
        service/service3 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] service0-65b8cdd65c-h784w cannot connect to service1
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   529.84ms  131.68ms   1.12s    76.60%
        Req/Sec   125.19     58.93   262.00     68.93%
        Latency Distribution
        50%  505.24ms
        75%  576.54ms
        90%  718.73ms
        99%  946.62ms
        2615 requests in 3.03s, 1.72MB read
        Requests/sec:    862.66
        Transfer/sec:    580.67KB
        [run.sh] Speed is 862.66, duration is 92
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d92s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service1-8497d67d4b-m628l        3m           13Mi
        service2-6d44f9d775-chmdz        58m          12Mi
        service3-6c849b8bcc-mx8nx        63m          12Mi
        ubuntu-client-76886f6bbd-wms78   4m           0Mi
        Running 2m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   491.25ms  121.85ms   1.65s    79.32%
        Req/Sec   130.58     52.65   313.00     69.84%
        Latency Distribution
        50%  476.28ms
        75%  539.92ms
        90%  621.88ms
        99%  923.61ms
        40000 requests in 1.53m, 26.28MB read
        Requests/sec:    434.78
        Transfer/sec:    292.54KB
        ------------------------------
        stop time: 38.185494
        stop time: 38.494138
        stop time: 38.761479
        stop time: 38.668739
        stop time: 38.453374
        stop time: 38.627025
        stop time: 38.792386
        stop time: 38.791629
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > command terminated with exit code 1
    [exp] Times: [38.185494, 38.494138, 38.761479, 38.668739, 38.453374, 38.627025, 38.792386, 38.791629]
    [exp] Throughput: 1036.3558019848442
[test.py] Baseline throughput: 1036.3558019848442
[test.py] Running groundtruth experiment
    [exp] Running (pre_run: False) workload synthetic/fanout-w3-http-async request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '30000000', 'PROCESSING_TIME_SERVICE0': '0.0', 'PROCESSING_TIME_SERVICE1': '0.00114078', 'PROCESSING_TIME_SERVICE2': '0.00034147', 'PROCESSING_TIME_SERVICE3': '0.00026614', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE3': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE3': '10'}
    [exp] Executing cmd `bash run.sh synthetic fanout-w3-http-async 8 512 40000`:
        [run.sh] Running benchmark synthetic with request fanout-w3-http-async, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        configmap "config-service3" deleted
        deployment.apps "service3" deleted
        service "service3" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        configmap/config-service3 created
        deployment.apps/service3 created
        service/service3 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   328.53ms   78.31ms 995.58ms   78.19%
        Req/Sec   189.59     57.14   390.00     75.00%
        Latency Distribution
        50%  320.20ms
        75%  356.86ms
        90%  405.63ms
        99%  616.42ms
        4408 requests in 3.03s, 2.90MB read
        Requests/sec:   1454.02
        Transfer/sec:      0.96MB
        [run.sh] Speed is 1454.02, duration is 55
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d55s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-65b8cdd65c-lfjdn        1585m        56Mi
        service1-8497d67d4b-ghdt5        1944m        17Mi
        service2-6d44f9d775-kw9n9        797m         11Mi
        service3-6c849b8bcc-qmq6w        688m         11Mi
        ubuntu-client-76886f6bbd-bvdlg   74m          12Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   334.97ms   55.66ms 870.33ms   86.31%
        Req/Sec   190.98     40.13   393.00     69.26%
        Latency Distribution
        50%  324.25ms
        75%  352.05ms
        90%  387.77ms
        99%  568.54ms
        40000 requests in 0.92m, 26.28MB read
        Requests/sec:    727.27
        Transfer/sec:    489.35KB
        ------------------------------
        stop time: 26.110438
        stop time: 26.304474
        stop time: 26.338492
        stop time: 26.364543
        stop time: 26.315919
        stop time: 26.472902
        stop time: 26.457906
        stop time: 26.424117
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [26.110438, 26.304474, 26.338492, 26.364543, 26.315919, 26.472902, 26.457906, 26.424117]
    [exp] Throughput: 1518.107288731496
    [exp] Running (pre_run: False) workload synthetic/fanout-w3-http-async request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '30000000', 'PROCESSING_TIME_SERVICE0': '0.000637', 'PROCESSING_TIME_SERVICE1': '0.00114078', 'PROCESSING_TIME_SERVICE2': '0.00034147', 'PROCESSING_TIME_SERVICE3': '0.00026614', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE3': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE3': '10'}
    [exp] Executing cmd `bash run.sh synthetic fanout-w3-http-async 8 512 40000`:
        [run.sh] Running benchmark synthetic with request fanout-w3-http-async, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        configmap "config-service3" deleted
        deployment.apps "service3" deleted
        service "service3" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        configmap/config-service3 created
        deployment.apps/service3 created
        service/service3 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   353.61ms   71.91ms 790.49ms   80.63%
        Req/Sec   182.26     51.63   313.00     78.57%
        Latency Distribution
        50%  339.67ms
        75%  377.47ms
        90%  434.03ms
        99%  641.09ms
        4103 requests in 3.03s, 2.70MB read
        Requests/sec:   1352.97
        Transfer/sec:      0.89MB
        [run.sh] Speed is 1352.97, duration is 59
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d59s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-65b8cdd65c-zvr6f        1920m        62Mi
        service1-8497d67d4b-wpqk7        1813m        15Mi
        service2-6d44f9d775-vjh26        521m         13Mi
        service3-6c849b8bcc-rcjp2        452m         13Mi
        ubuntu-client-76886f6bbd-72j7j   77m          12Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   339.05ms   62.41ms   1.01s    80.77%
        Req/Sec   188.92     43.71   363.00     69.96%
        Latency Distribution
        50%  330.37ms
        75%  363.96ms
        90%  404.20ms
        99%  548.04ms
        40000 requests in 0.98m, 26.28MB read
        Requests/sec:    677.96
        Transfer/sec:    456.18KB
        ------------------------------
        stop time: 26.507167
        stop time: 26.645780
        stop time: 26.639001
        stop time: 26.610051
        stop time: 26.810397
        stop time: 26.704706
        stop time: 26.813351
        stop time: 26.713742
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [26.507167, 26.64578, 26.639001, 26.610051, 26.810397, 26.704706, 26.813351, 26.713742]
    [exp] Throughput: 1499.2209087719625
[test.py] Running slowdown experiment
    [exp] Running (pre_run: False) workload synthetic/fanout-w3-http-async request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '30000000', 'PROCESSING_TIME_SERVICE0': '0.00127591', 'PROCESSING_TIME_SERVICE1': '0.00114078', 'PROCESSING_TIME_SERVICE2': '0.00034147', 'PROCESSING_TIME_SERVICE3': '0.00026614', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '637.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '63750000', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '637.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '63750000', 'SLOWPOKE_DELAY_MICROS_SERVICE3': '637.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE3': '63750000'}
    [exp] Executing cmd `bash run.sh synthetic fanout-w3-http-async 8 512 40000`:
        [run.sh] Running benchmark synthetic with request fanout-w3-http-async, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        configmap "config-service3" deleted
        deployment.apps "service3" deleted
        service "service3" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        configmap/config-service3 created
        deployment.apps/service3 created
        service/service3 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] service0-65b8cdd65c-jwx8k cannot connect to service1
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   646.33ms  147.35ms   1.33s    79.91%
        Req/Sec    96.92     49.02   240.00     70.83%
        Latency Distribution
        50%  623.44ms
        75%  690.03ms
        90%  835.30ms
        99%    1.16s
        2120 requests in 3.02s, 1.39MB read
        Requests/sec:    703.00
        Transfer/sec:    473.68KB
        [run.sh] Speed is 703.00, duration is 113
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d113s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        Running 2m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   654.71ms  105.11ms   1.56s    80.89%
        Req/Sec    97.83     33.96   350.00     70.01%
        Latency Distribution
        50%  638.94ms
        75%  684.97ms
        90%  771.99ms
        99%    1.04s
        40000 requests in 1.88m, 26.28MB read
        Requests/sec:    353.98
        Transfer/sec:    238.18KB
        ------------------------------
        stop time: 51.200239
        stop time: 51.097689
        stop time: 51.105226
        stop time: 51.460559
        stop time: 51.513972
        stop time: 51.850327
        stop time: 51.943971
        stop time: 51.710726
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > command terminated with exit code 1
        > error: metrics not available yet
    [exp] Times: [51.200239, 51.097689, 51.105226, 51.460559, 51.513972, 51.850327, 51.943971, 51.710726]
    [exp] Throughput: 776.9202081265324
    [exp] Running (pre_run: False) workload synthetic/fanout-w3-http-async request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '30000000', 'PROCESSING_TIME_SERVICE0': '0.00127591', 'PROCESSING_TIME_SERVICE1': '0.00114078', 'PROCESSING_TIME_SERVICE2': '0.00034147', 'PROCESSING_TIME_SERVICE3': '0.00026614', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '319.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '31900000', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '319.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '31900000', 'SLOWPOKE_DELAY_MICROS_SERVICE3': '319.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE3': '31900000'}
    [exp] Executing cmd `bash run.sh synthetic fanout-w3-http-async 8 512 40000`:
        [run.sh] Running benchmark synthetic with request fanout-w3-http-async, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        configmap "config-service3" deleted
        deployment.apps "service3" deleted
        service "service3" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        configmap/config-service3 created
        deployment.apps/service3 created
        service/service3 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   567.95ms  258.08ms   1.73s    85.21%
        Req/Sec   111.01     49.62   232.00     63.06%
        Latency Distribution
        50%  504.54ms
        75%  579.30ms
        90%  828.25ms
        99%    1.57s
        2502 requests in 3.03s, 1.64MB read
        Requests/sec:    825.49
        Transfer/sec:    555.43KB
        [run.sh] Speed is 825.49, duration is 96
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d96s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        Running 2m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   508.86ms   88.26ms   1.74s    83.72%
        Req/Sec   125.83     36.86   262.00     73.35%
        Latency Distribution
        50%  499.11ms
        75%  541.96ms
        90%  592.53ms
        99%  780.14ms
        40000 requests in 1.60m, 26.28MB read
        Requests/sec:    416.67
        Transfer/sec:    280.35KB
        ------------------------------
        stop time: 39.818152
        stop time: 39.875469
        stop time: 39.901545
        stop time: 39.865876
        stop time: 40.062149
        stop time: 40.196005
        stop time: 40.146310
        stop time: 40.290488
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > error: metrics not available yet
    [exp] Times: [39.818152, 39.875469, 39.901545, 39.865876, 40.062149, 40.196005, 40.14631, 40.290488]
    [exp] Throughput: 999.5127562721816
[test.py] Baseline throughput:  1036.3558019848442
[test.py] Groundtruth:  [1518.107288731496, 1499.2209087719625]
[test.py] Slowdown:  [776.9202081265324, 999.5127562721816]
[test.py] Predicted:  [1540.4084592319996, 1468.35874577458]
[test.py] Error percentage:  [1.4690114899018765, -2.0585467302922162]
[test.py] ++++++++++++++++++++++++++++++++ Summary: 
[test.py] Result for the experiment 0: 
    Baseline throughput: 1036.3558019848442
    Groundtruth: [1518.107288731496, 1499.2209087719625]
    Slowdown:    [776.9202081265324, 999.5127562721816]
    Predicted:   [1540.4084592319996, 1468.35874577458]
    Error Perc:  [1.4690114899018765, -2.0585467302922162]
