[config.py] Random numbers for execution time: [247.2573382140748, 476.70040537096673, 676.5993667502922]
benchmark                        : synthetic
num_threads                      : 8
num_conns                        : 128
target_service                   : service0
request_type                     : chain-d2-http-sync
repetitions                      : 1
target_num_exp                   : 2
pre_run                          : False
num_req                          : 40000
groundtruths                     : []
slowdowns                        : []
predicteds                       : []
errs                             : []
client_cpu_quota                 : 2
random_seed                      : 4446
request_ratio                    : {'service0': 1, 'service1': 1, 'service2': 1}
baseline_service_processing_time : {'service0': 676.6, 'service1': 476.7, 'service2': 247.26}
cpu_quota                        : {'service0': 2, 'service1': 2, 'service2': 2}
target_processing_time_range     : [0, 676.6]
baseline_throughputs             : []
poker_batch                      : 40000000
[test.py] Starting experiment...
[test.py] >>>>>>>>>>>>>>>>>>>>>>>>>>>>> Running experiment 0...
[test.py] Actual processing time range: [0, 338]
[test.py] Running baseline experiment
    [exp] Running (pre_run: False) workload synthetic/chain-d2-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '40000000', 'PROCESSING_TIME_SERVICE0': '0.0006766', 'PROCESSING_TIME_SERVICE1': '0.0004767', 'PROCESSING_TIME_SERVICE2': '0.00024726', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0'}
    [exp] Executing cmd `bash run.sh synthetic chain-d2-http-sync 8 128 40000`:
        [run.sh] Running benchmark synthetic with request chain-d2-http-sync, thread 8, conn 128, duration 60
        [run.sh] Deleting all services
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c128 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 128 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency    90.89ms   50.91ms 400.14ms   70.32%
        Req/Sec   178.77     49.30   500.00     79.15%
        Latency Distribution
        50%   82.83ms
        75%  118.74ms
        90%  157.45ms
        99%  250.42ms
        4273 requests in 3.10s, 3.00MB read
        Requests/sec:   1380.45
        Transfer/sec:      0.97MB
        [run.sh] Speed is 1380.45, duration is 86
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c128 -d86s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-74c66db75c-lnd5l        1945m        50Mi
        service1-6bcd7fbfb7-fnsdd        1399m        44Mi
        service2-6dc44f9545-dwplf        451m         9Mi
        ubuntu-client-76886f6bbd-jd6x8   31m          9Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 128 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   352.86ms  625.58ms   6.67s    90.22%
        Req/Sec   104.91     71.43   323.00     56.33%
        Latency Distribution
        50%  108.98ms
        75%  322.76ms
        90%  959.83ms
        99%    3.13s
        40000 requests in 1.43m, 28.11MB read
        Requests/sec:    465.12
        Transfer/sec:    334.67KB
        ------------------------------
        stop time: 61.092226
        stop time: 53.742303
        stop time: 57.353191
        stop time: 60.681482
        stop time: 57.171299
        stop time: 59.177312
        stop time: 57.171978
        stop time: 60.972344
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [61.092226, 53.742303, 57.353191, 60.681482, 57.171299, 59.177312, 57.171978, 60.972344]
    [exp] Throughput: 684.6938937404503
[test.py] Baseline throughput: 684.6938937404503
[test.py] Running groundtruth experiment
    [exp] Running (pre_run: False) workload synthetic/chain-d2-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '40000000', 'PROCESSING_TIME_SERVICE0': '0.0', 'PROCESSING_TIME_SERVICE1': '0.0004767', 'PROCESSING_TIME_SERVICE2': '0.00024726', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0'}
    [exp] Executing cmd `bash run.sh synthetic chain-d2-http-sync 8 128 40000`:
        [run.sh] Running benchmark synthetic with request chain-d2-http-sync, thread 8, conn 128, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c128 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 128 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency    81.37ms   61.94ms 485.70ms   73.04%
        Req/Sec   209.45     56.52   450.00     69.42%
        Latency Distribution
        50%   67.93ms
        75%  112.76ms
        90%  168.47ms
        99%  277.56ms
        5051 requests in 3.09s, 3.51MB read
        Requests/sec:   1634.75
        Transfer/sec:      1.14MB
        [run.sh] Speed is 1634.75, duration is 73
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c128 -d73s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-74c66db75c-gf84w        69m          24Mi
        service1-6bcd7fbfb7-sjzh9        752m         20Mi
        service2-6dc44f9545-wn2lk        34m          10Mi
        ubuntu-client-76886f6bbd-pq2w4   6m           6Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 128 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   758.43ms    1.89s   12.79s    91.89%
        Req/Sec   117.25     76.57   373.00     61.58%
        Latency Distribution
        50%   97.68ms
        75%  417.60ms
        90%    1.59s
        99%    9.59s
        40000 requests in 1.22m, 27.81MB read
        Requests/sec:    547.94
        Transfer/sec:    390.10KB
        ------------------------------
        stop time: 52.750842
        stop time: 60.302374
        stop time: 54.168639
        stop time: 56.067863
        stop time: 52.614833
        stop time: 52.570768
        stop time: 59.406500
        stop time: 53.678317
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [52.750842, 60.302374, 54.168639, 56.067863, 52.614833, 52.570768, 59.4065, 53.678317]
    [exp] Throughput: 724.7031013687341
    [exp] Running (pre_run: False) workload synthetic/chain-d2-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '40000000', 'PROCESSING_TIME_SERVICE0': '0.000338', 'PROCESSING_TIME_SERVICE1': '0.0004767', 'PROCESSING_TIME_SERVICE2': '0.00024726', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0'}
    [exp] Executing cmd `bash run.sh synthetic chain-d2-http-sync 8 128 40000`:
        [run.sh] Running benchmark synthetic with request chain-d2-http-sync, thread 8, conn 128, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c128 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 128 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency    83.45ms   54.52ms 321.10ms   68.96%
        Req/Sec   196.49     51.09   404.00     69.55%
        Latency Distribution
        50%   74.66ms
        75%  112.79ms
        90%  158.73ms
        99%  253.81ms
        4757 requests in 3.10s, 3.34MB read
        Requests/sec:   1534.96
        Transfer/sec:      1.08MB
        [run.sh] Speed is 1534.96, duration is 78
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c128 -d78s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-74c66db75c-srtkq        475m         16Mi
        service2-6dc44f9545-fxd96        270m         9Mi
        ubuntu-client-76886f6bbd-pgmww   17m          0Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 128 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   257.74ms  408.26ms   4.16s    89.94%
        Req/Sec   105.37     76.38   580.00     66.16%
        Latency Distribution
        50%  100.23ms
        75%  296.41ms
        90%  669.52ms
        99%    1.92s
        40001 requests in 1.30m, 28.09MB read
        Requests/sec:    512.83
        Transfer/sec:    368.72KB
        ------------------------------
        stop time: 53.091365
        stop time: 52.689488
        stop time: 52.857411
        stop time: 53.068740
        stop time: 52.864070
        stop time: 52.702324
        stop time: 52.760154
        stop time: 53.023106
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [53.091365, 52.689488, 52.857411, 53.06874, 52.86407, 52.702324, 52.760154, 53.023106]
    [exp] Throughput: 756.3998673671745
[test.py] Running slowdown experiment
    [exp] Running (pre_run: False) workload synthetic/chain-d2-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '40000000', 'PROCESSING_TIME_SERVICE0': '0.0006766', 'PROCESSING_TIME_SERVICE1': '0.0004767', 'PROCESSING_TIME_SERVICE2': '0.00024726', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '338.0', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '338.0'}
    [exp] Executing cmd `bash run.sh synthetic chain-d2-http-sync 8 128 40000`:
        [run.sh] Running benchmark synthetic with request chain-d2-http-sync, thread 8, conn 128, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c128 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 128 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   117.44ms   59.90ms 369.09ms   69.04%
        Req/Sec   134.92     46.72   282.00     73.33%
        Latency Distribution
        50%  109.01ms
        75%  153.30ms
        90%  197.74ms
        99%  296.57ms
        3232 requests in 3.02s, 2.27MB read
        Requests/sec:   1071.65
        Transfer/sec:    771.29KB
        [run.sh] Speed is 1071.65, duration is 111
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c128 -d111s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        Running 2m test @ http://service0:80/endpoint1
        8 threads and 128 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   259.81ms  383.80ms   4.46s    91.46%
        Req/Sec    87.10     51.41   333.00     64.09%
        Latency Distribution
        50%  137.23ms
        75%  257.25ms
        90%  577.71ms
        99%    2.16s
        40000 requests in 1.85m, 28.12MB read
        Requests/sec:    360.36
        Transfer/sec:    259.40KB
        ------------------------------
        stop time: 63.324551
        stop time: 63.026179
        stop time: 62.713136
        stop time: 62.346383
        stop time: 62.491372
        stop time: 63.350608
        stop time: 63.192013
        stop time: 63.408423
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > error: metrics not available yet
    [exp] Times: [63.324551, 63.026179, 62.713136, 62.346383, 62.491372, 63.350608, 63.192013, 63.408423]
    [exp] Throughput: 635.1062964011513
    [exp] Running (pre_run: False) workload synthetic/chain-d2-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '40000000', 'PROCESSING_TIME_SERVICE0': '0.0006766', 'PROCESSING_TIME_SERVICE1': '0.0004767', 'PROCESSING_TIME_SERVICE2': '0.00024726', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '169.0', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '169.0'}
    [exp] Executing cmd `bash run.sh synthetic chain-d2-http-sync 8 128 40000`:
        [run.sh] Running benchmark synthetic with request chain-d2-http-sync, thread 8, conn 128, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c128 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 128 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   104.65ms   54.10ms 384.14ms   70.64%
        Req/Sec   153.93     51.56   343.00     72.73%
        Latency Distribution
        50%   97.24ms
        75%  133.23ms
        90%  177.23ms
        99%  264.92ms
        3714 requests in 3.10s, 2.61MB read
        Requests/sec:   1199.48
        Transfer/sec:    863.30KB
        [run.sh] Speed is 1199.48, duration is 100
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c128 -d100s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-74c66db75c-gnnzp        370m         24Mi
        service1-74dc665c69-h9pft        469m         16Mi
        service2-577cbd8655-bclj6        137m         10Mi
        ubuntu-client-76886f6bbd-wk7vr   15m          5Mi
        Running 2m test @ http://service0:80/endpoint1
        8 threads and 128 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   200.82ms  241.34ms   2.52s    90.19%
        Req/Sec    96.15     55.62   282.00     63.98%
        Latency Distribution
        50%  124.39ms
        75%  224.52ms
        90%  437.20ms
        99%    1.29s
        40000 requests in 1.67m, 28.12MB read
        Requests/sec:    400.00
        Transfer/sec:    287.90KB
        ------------------------------
        stop time: 56.178058
        stop time: 53.425425
        stop time: 53.605797
        stop time: 57.226284
        stop time: 53.552148
        stop time: 54.677436
        stop time: 56.095432
        stop time: 55.860678
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [56.178058, 53.425425, 53.605797, 57.226284, 53.552148, 54.677436, 56.095432, 55.860678]
    [exp] Throughput: 726.2473023941119
[test.py] Baseline throughput:  684.6938937404503
[test.py] Groundtruth:  [724.7031013687341, 756.3998673671745]
[test.py] Slowdown:  [635.1062964011513, 726.2473023941119]
[test.py] Predicted:  [808.9046958977776, 828.0603613979395]
[test.py] Error percentage:  [11.61877110364416, 9.473890348526368]
[test.py] ++++++++++++++++++++++++++++++++ Summary: 
[test.py] Result for the experiment 0: 
    Baseline throughput: 684.6938937404503
    Groundtruth: [724.7031013687341, 756.3998673671745]
    Slowdown:    [635.1062964011513, 726.2473023941119]
    Predicted:   [808.9046958977776, 828.0603613979395]
    Error Perc:  [11.61877110364416, 9.473890348526368]
