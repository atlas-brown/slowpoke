[config.py] Random numbers for execution time: [266.13634741120245, 341.46867326669883, 1140.7815626331162, 1275.9127585791719]
benchmark                        : synthetic
num_threads                      : 8
num_conns                        : 512
target_service                   : service0
request_type                     : fanout-w3-http-sync
repetitions                      : 1
target_num_exp                   : 2
pre_run                          : False
num_req                          : 40000
groundtruths                     : []
slowdowns                        : []
predicteds                       : []
errs                             : []
client_cpu_quota                 : 2
random_seed                      : 24479
request_ratio                    : {'service0': 1, 'service1': 1, 'service2': 1, 'service3': 1}
baseline_service_processing_time : {'service0': 1275.91, 'service1': 1140.78, 'service2': 341.47, 'service3': 266.14}
cpu_quota                        : {'service0': 2, 'service1': 2, 'service2': 2, 'service3': 2}
target_processing_time_range     : [0, 1275.91]
baseline_throughputs             : []
poker_batch                      : 30000000
[test.py] Starting experiment...
[test.py] >>>>>>>>>>>>>>>>>>>>>>>>>>>>> Running experiment 0...
[test.py] Actual processing time range: [0, 637]
[test.py] Running baseline experiment
    [exp] Running (pre_run: False) workload synthetic/fanout-w3-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '30000000', 'PROCESSING_TIME_SERVICE0': '0.00127591', 'PROCESSING_TIME_SERVICE1': '0.00114078', 'PROCESSING_TIME_SERVICE2': '0.00034147', 'PROCESSING_TIME_SERVICE3': '0.00026614', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE3': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE3': '10'}
    [exp] Executing cmd `bash run.sh synthetic fanout-w3-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request fanout-w3-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        configmap/config-service3 created
        deployment.apps/service3 created
        service/service3 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   504.53ms  140.97ms   1.06s    75.15%
        Req/Sec   127.74     47.50   232.00     70.19%
        Latency Distribution
        50%  469.86ms
        75%  557.41ms
        90%  715.36ms
        99%  949.20ms
        2724 requests in 3.04s, 1.79MB read
        Requests/sec:    897.38
        Transfer/sec:    604.02KB
        [run.sh] Speed is 897.38, duration is 89
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d89s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-5f7bfff576-n7mjl        607m         52Mi
        service1-96558989f-7td62         415m         13Mi
        service2-b9b68cb9d-nx2d7         166m         12Mi
        service3-8798dc54d-ns5zf         142m         12Mi
        ubuntu-client-76886f6bbd-5xl2c   26m          16Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   467.31ms  185.65ms   2.29s    77.79%
        Req/Sec   137.11     45.24   390.00     68.21%
        Latency Distribution
        50%  428.23ms
        75%  536.87ms
        90%  703.41ms
        99%    1.10s
        40000 requests in 1.48m, 26.28MB read
        Requests/sec:    449.44
        Transfer/sec:    302.41KB
        ------------------------------
        stop time: 36.451020
        stop time: 36.823493
        stop time: 36.559660
        stop time: 36.338384
        stop time: 36.778779
        stop time: 36.930102
        stop time: 36.876647
        stop time: 36.993419
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [36.45102, 36.823493, 36.55966, 36.338384, 36.778779, 36.930102, 36.876647, 36.993419]
    [exp] Throughput: 1089.3561246243014
[test.py] Baseline throughput: 1089.3561246243014
[test.py] Running groundtruth experiment
    [exp] Running (pre_run: False) workload synthetic/fanout-w3-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '30000000', 'PROCESSING_TIME_SERVICE0': '0.0', 'PROCESSING_TIME_SERVICE1': '0.00114078', 'PROCESSING_TIME_SERVICE2': '0.00034147', 'PROCESSING_TIME_SERVICE3': '0.00026614', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE3': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE3': '10'}
    [exp] Executing cmd `bash run.sh synthetic fanout-w3-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request fanout-w3-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        configmap "config-service3" deleted
        deployment.apps "service3" deleted
        service "service3" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        configmap/config-service3 created
        deployment.apps/service3 created
        service/service3 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   330.90ms   87.65ms 924.15ms   83.08%
        Req/Sec   184.01     61.10   383.00     72.92%
        Latency Distribution
        50%  320.51ms
        75%  357.91ms
        90%  416.92ms
        99%  644.95ms
        4401 requests in 3.03s, 2.89MB read
        Requests/sec:   1451.42
        Transfer/sec:      0.95MB
        [run.sh] Speed is 1451.42, duration is 55
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d55s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-5f7bfff576-f9vqs        973m         47Mi
        service1-96558989f-l9qdp         1541m        16Mi
        service2-b9b68cb9d-v6bxv         595m         10Mi
        service3-8798dc54d-q6njc         509m         10Mi
        ubuntu-client-76886f6bbd-7b2fb   74m          12Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   334.60ms   63.07ms 804.50ms   87.41%
        Req/Sec   190.85     45.57   460.00     71.15%
        Latency Distribution
        50%  322.15ms
        75%  352.15ms
        90%  393.10ms
        99%  597.66ms
        40000 requests in 0.92m, 26.28MB read
        Requests/sec:    727.27
        Transfer/sec:    489.35KB
        ------------------------------
        stop time: 26.237787
        stop time: 26.201738
        stop time: 26.421975
        stop time: 26.383551
        stop time: 26.300931
        stop time: 26.243716
        stop time: 26.492660
        stop time: 26.380131
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [26.237787, 26.201738, 26.421975, 26.383551, 26.300931, 26.243716, 26.49266, 26.380131]
    [exp] Throughput: 1519.0174649460255
    [exp] Running (pre_run: False) workload synthetic/fanout-w3-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '30000000', 'PROCESSING_TIME_SERVICE0': '0.000637', 'PROCESSING_TIME_SERVICE1': '0.00114078', 'PROCESSING_TIME_SERVICE2': '0.00034147', 'PROCESSING_TIME_SERVICE3': '0.00026614', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE3': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE3': '10'}
    [exp] Executing cmd `bash run.sh synthetic fanout-w3-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request fanout-w3-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        configmap "config-service3" deleted
        deployment.apps "service3" deleted
        service "service3" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        configmap/config-service3 created
        deployment.apps/service3 created
        service/service3 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   357.55ms   95.71ms 857.49ms   76.42%
        Req/Sec   173.31     64.48   313.00     66.67%
        Latency Distribution
        50%  342.83ms
        75%  396.19ms
        90%  481.22ms
        99%  660.24ms
        4067 requests in 3.03s, 2.67MB read
        Requests/sec:   1341.59
        Transfer/sec:      0.88MB
        [run.sh] Speed is 1341.59, duration is 59
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d59s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   332.89ms   63.64ms   1.05s    84.35%
        Req/Sec   192.36     44.11   340.00     69.92%
        Latency Distribution
        50%  323.06ms
        75%  352.24ms
        90%  391.28ms
        99%  591.41ms
        40000 requests in 0.98m, 26.28MB read
        Requests/sec:    677.96
        Transfer/sec:    456.17KB
        ------------------------------
        stop time: 26.125832
        stop time: 26.074410
        stop time: 26.131900
        stop time: 26.306135
        stop time: 26.236624
        stop time: 26.271396
        stop time: 26.211101
        stop time: 26.144895
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > error: metrics not available yet
    [exp] Times: [26.125832, 26.07441, 26.1319, 26.306135, 26.236624, 26.271396, 26.211101, 26.144895]
    [exp] Throughput: 1527.4295828351628
[test.py] Running slowdown experiment
    [exp] Running (pre_run: False) workload synthetic/fanout-w3-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '30000000', 'PROCESSING_TIME_SERVICE0': '0.00127591', 'PROCESSING_TIME_SERVICE1': '0.00114078', 'PROCESSING_TIME_SERVICE2': '0.00034147', 'PROCESSING_TIME_SERVICE3': '0.00026614', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '637.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '63750000', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '637.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '63750000', 'SLOWPOKE_DELAY_MICROS_SERVICE3': '637.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE3': '63750000'}
    [exp] Executing cmd `bash run.sh synthetic fanout-w3-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request fanout-w3-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        configmap "config-service3" deleted
        deployment.apps "service3" deleted
        service "service3" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        configmap/config-service3 created
        deployment.apps/service3 created
        service/service3 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   664.45ms  181.88ms   1.31s    79.80%
        Req/Sec    90.86     45.44   212.00     62.39%
        Latency Distribution
        50%  637.10ms
        75%  751.66ms
        90%  901.35ms
        99%    1.22s
        2020 requests in 3.03s, 1.33MB read
        Requests/sec:    666.98
        Transfer/sec:    448.78KB
        [run.sh] Speed is 666.98, duration is 119
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d119s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        Running 2m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   657.65ms  120.44ms   1.52s    75.21%
        Req/Sec    97.34     38.80   250.00     68.83%
        Latency Distribution
        50%  641.13ms
        75%  696.73ms
        90%  800.71ms
        99%    1.08s
        40001 requests in 1.98m, 26.28MB read
        Requests/sec:    336.14
        Transfer/sec:    226.17KB
        ------------------------------
        stop time: 51.546846
        stop time: 51.758887
        stop time: 51.781630
        stop time: 51.790222
        stop time: 51.758386
        stop time: 51.602624
        stop time: 51.846764
        stop time: 51.616137
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > error: metrics not available yet
    [exp] Times: [51.546846, 51.758887, 51.78163, 51.790222, 51.758386, 51.602624, 51.846764, 51.616137]
    [exp] Throughput: 773.5045753859202
    [exp] Running (pre_run: False) workload synthetic/fanout-w3-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '30000000', 'PROCESSING_TIME_SERVICE0': '0.00127591', 'PROCESSING_TIME_SERVICE1': '0.00114078', 'PROCESSING_TIME_SERVICE2': '0.00034147', 'PROCESSING_TIME_SERVICE3': '0.00026614', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '319.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '31900000', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '319.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '31900000', 'SLOWPOKE_DELAY_MICROS_SERVICE3': '319.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE3': '31900000'}
    [exp] Executing cmd `bash run.sh synthetic fanout-w3-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request fanout-w3-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        configmap "config-service3" deleted
        deployment.apps "service3" deleted
        service "service3" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        configmap/config-service3 created
        deployment.apps/service3 created
        service/service3 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   548.24ms  137.16ms   1.23s    76.97%
        Req/Sec   120.85     47.88   250.00     68.75%
        Latency Distribution
        50%  511.94ms
        75%  593.21ms
        90%  750.71ms
        99%    1.01s
        2570 requests in 3.03s, 1.69MB read
        Requests/sec:    847.53
        Transfer/sec:    570.48KB
        [run.sh] Speed is 847.53, duration is 94
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d94s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                        CPU(cores)   MEMORY(bytes)
        service0-5f7bfff576-7mdm6   349m         52Mi
        service1-5ddc595896-chm8m   3m           14Mi
        service2-6cd764949-bhn86    53m          12Mi
        service3-68f8bdff4c-vmgzb   44m          12Mi
        Running 2m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   494.68ms   76.91ms   1.11s    73.99%
        Req/Sec   129.40     40.77   282.00     70.94%
        Latency Distribution
        50%  486.56ms
        75%  534.11ms
        90%  586.39ms
        99%  738.51ms
        40000 requests in 1.57m, 26.28MB read
        Requests/sec:    425.53
        Transfer/sec:    286.32KB
        ------------------------------
        stop time: 38.743924
        stop time: 38.827258
        stop time: 38.900004
        stop time: 38.885911
        stop time: 39.005433
        stop time: 39.097768
        stop time: 38.947246
        stop time: 38.868655
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [38.743924, 38.827258, 38.900004, 38.885911, 39.005433, 39.097768, 38.947246, 38.868655]
    [exp] Throughput: 1028.025917265843
[test.py] Baseline throughput:  1089.3561246243014
[test.py] Groundtruth:  [1519.0174649460255, 1527.4295828351628]
[test.py] Slowdown:  [773.5045753859202, 1028.025917265843]
[test.py] Predicted:  [1527.0388765391738, 1530.7298880306012]
[test.py] Error percentage:  [0.528065791095648, 0.21606922063879544]
[test.py] ++++++++++++++++++++++++++++++++ Summary: 
[test.py] Result for the experiment 0: 
    Baseline throughput: 1089.3561246243014
    Groundtruth: [1519.0174649460255, 1527.4295828351628]
    Slowdown:    [773.5045753859202, 1028.025917265843]
    Predicted:   [1527.0388765391738, 1530.7298880306012]
    Error Perc:  [0.528065791095648, 0.21606922063879544]
