[config.py] Random numbers for execution time: [786.0420416734107, 826.5145977008433, 958.0749646410659]
benchmark                        : synthetic
num_threads                      : 8
num_conns                        : 512
target_service                   : service2
request_type                     : dynamic-cache-http-sync
repetitions                      : 1
target_num_exp                   : 10
pre_run                          : False
num_req                          : 40000
groundtruths                     : []
slowdowns                        : []
predicteds                       : []
errs                             : []
client_cpu_quota                 : 2
random_seed                      : 13505
request_ratio                    : {'service0': 1, 'service1': 1, 'service2': 0.25}
baseline_service_processing_time : {'service0': 958.07, 'service1': 826.51, 'service2': 6288.34}
cpu_quota                        : {'service0': 2, 'service1': 2, 'service2': 2}
target_processing_time_range     : [0, 6288.34]
baseline_throughputs             : []
poker_batch                      : 20000000
[test.py] Starting experiment...
[test.py] >>>>>>>>>>>>>>>>>>>>>>>>>>>>> Running experiment 0...
[test.py] Actual processing time range: [0, 628, 1256, 1884, 2512, 3140, 3768, 4396, 5024, 5652]
[test.py] Running baseline experiment
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.00628834', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] service0-7564bb8985-gn9c7 cannot connect to service1
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   437.23ms  153.84ms   1.26s    78.22%
        Req/Sec   137.00     54.53   280.00     70.39%
        Latency Distribution
        50%  370.07ms
        75%  494.48ms
        90%  685.59ms
        99%  927.98ms
        3201 requests in 3.03s, 1.22MB read
        Requests/sec:   1056.07
        Transfer/sec:    412.58KB
        [run.sh] Speed is 1056.07, duration is 56
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d56s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   429.64ms  161.01ms   1.77s    76.27%
        Req/Sec   149.22     49.73   620.00     70.93%
        Latency Distribution
        50%  366.98ms
        75%  454.97ms
        90%  704.33ms
        99%  843.21ms
        40000 requests in 0.93m, 15.34MB read
        Requests/sec:    714.28
        Transfer/sec:    280.43KB
        ------------------------------
        stop time: 33.117112
        stop time: 33.026546
        stop time: 33.572113
        stop time: 33.913786
        stop time: 33.918121
        stop time: 34.134327
        stop time: 34.206391
        stop time: 34.168454
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > command terminated with exit code 1
        > error: metrics not available yet
    [exp] Times: [33.117112, 33.026546, 33.572113, 33.913786, 33.918121, 34.134327, 34.206391, 34.168454]
    [exp] Throughput: 1184.935690392597
[test.py] Baseline throughput: 1184.935690392597
[test.py] Running 0th optmization experiment
[test.py] Running 0th groundtruth exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.0', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] service0-7564bb8985-hjdl2 cannot connect to service1
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   348.19ms  160.01ms   1.28s    85.43%
        Req/Sec   181.66     61.14   300.00     66.52%
        Latency Distribution
        50%  309.70ms
        75%  358.04ms
        90%  502.91ms
        99%    1.09s
        4276 requests in 3.02s, 1.64MB read
        Requests/sec:   1414.48
        Transfer/sec:    556.83KB
        [run.sh] Speed is 1414.48, duration is 42
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d42s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                        CPU(cores)   MEMORY(bytes)
        service0-7564bb8985-hjdl2   59m          38Mi
        service1-5b899d549d-flzpl   490m         15Mi
        service2-6f7c796b99-xqd97   27m          8Mi
        Running 42s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   316.32ms   94.25ms   2.04s    88.86%
        Req/Sec   203.10     48.45   560.00     72.68%
        Latency Distribution
        50%  301.37ms
        75%  337.20ms
        90%  386.60ms
        99%  677.14ms
        40000 requests in 42.00s, 15.34MB read
        Requests/sec:    952.38
        Transfer/sec:    374.01KB
        ------------------------------
        stop time: 24.170951
        stop time: 24.493012
        stop time: 24.742375
        stop time: 24.959321
        stop time: 25.010038
        stop time: 24.907251
        stop time: 24.934767
        stop time: 24.960476
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > command terminated with exit code 1
    [exp] Times: [24.170951, 24.493012, 24.742375, 24.959321, 25.010038, 24.907251, 24.934767, 24.960476]
    [exp] Throughput: 1614.7084519506993
[test.py] Running 0th slowdown exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.00628834', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '786.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '78599990', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '786.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '78599990', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   694.58ms  186.07ms   1.59s    79.58%
        Req/Sec    87.45     54.30   330.00     64.79%
        Latency Distribution
        50%  693.31ms
        75%  757.23ms
        90%  877.14ms
        99%    1.44s
        1969 requests in 3.03s, 773.28KB read
        Requests/sec:    649.75
        Transfer/sec:    255.17KB
        [run.sh] Speed is 649.75, duration is 92
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d92s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-64989cf7f-xgw7r         904m         49Mi
        service1-57b8c9bb4d-6wh6m        720m         17Mi
        service2-6f7c796b99-sgmps        1170m        10Mi
        ubuntu-client-76886f6bbd-dzwk4   31m          14Mi
        Running 2m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   718.82ms  114.83ms   2.08s    71.48%
        Req/Sec    89.95     45.08   360.00     64.63%
        Latency Distribution
        50%  705.21ms
        75%  769.17ms
        90%  848.32ms
        99%    1.02s
        40000 requests in 1.53m, 15.37MB read
        Requests/sec:    434.78
        Transfer/sec:    171.04KB
        ------------------------------
        stop time: 55.733363
        stop time: 55.972774
        stop time: 56.404878
        stop time: 56.623935
        stop time: 56.765613
        stop time: 56.643341
        stop time: 56.868981
        stop time: 56.859039
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [55.733363, 55.972774, 56.404878, 56.623935, 56.765613, 56.643341, 56.868981, 56.859039]
    [exp] Throughput: 708.1652632173711
[test.py] Finished running 0th optmization experiment: groundtruth->1614.7084519506993, slowdown->708.1652632173711, predicted->1597.2979787931135, err->-1.0782425233826276
[test.py] Running 1th optmization experiment
[test.py] Running 1th groundtruth exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.000628', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   337.52ms   83.13ms 727.04ms   80.76%
        Req/Sec   194.26     55.53   330.00     71.76%
        Latency Distribution
        50%  313.07ms
        75%  365.85ms
        90%  455.30ms
        99%  623.87ms
        4278 requests in 3.03s, 1.64MB read
        Requests/sec:   1411.51
        Transfer/sec:    553.43KB
        [run.sh] Speed is 1411.51, duration is 42
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d42s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        Running 42s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   317.78ms   86.39ms   1.60s    88.09%
        Req/Sec   202.13     47.35   550.00     68.27%
        Latency Distribution
        50%  302.57ms
        75%  338.72ms
        90%  389.38ms
        99%  639.30ms
        40000 requests in 42.00s, 15.35MB read
        Requests/sec:    952.38
        Transfer/sec:    374.14KB
        ------------------------------
        stop time: 24.469653
        stop time: 24.595353
        stop time: 24.923962
        stop time: 24.946781
        stop time: 25.062624
        stop time: 25.118159
        stop time: 25.065873
        stop time: 25.059742
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > error: metrics not available yet
    [exp] Times: [24.469653, 24.595353, 24.923962, 24.946781, 25.062624, 25.118159, 25.065873, 25.059742]
    [exp] Throughput: 1606.0858850311424
[test.py] Running 1th slowdown exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.00628834', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '707.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '70749990', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '707.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '70749990', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] service0-86dc876448-pf4wl cannot connect to service1
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   659.03ms  217.57ms   1.78s    78.50%
        Req/Sec    88.62     48.02   212.00     59.38%
        Latency Distribution
        50%  661.25ms
        75%  711.77ms
        90%  883.21ms
        99%    1.40s
        2019 requests in 3.02s, 823.23KB read
        Requests/sec:    667.46
        Transfer/sec:    272.15KB
        [run.sh] Speed is 667.46, duration is 89
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d89s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-86dc876448-pf4wl        580m         47Mi
        service1-c9dbd999-8wr2z          64m          17Mi
        service2-6f7c796b99-48sqb        39m          11Mi
        ubuntu-client-76886f6bbd-n76cw   9m           14Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   671.55ms  119.99ms   2.02s    77.90%
        Req/Sec    95.89     45.49   313.00     64.88%
        Latency Distribution
        50%  664.49ms
        75%  707.47ms
        90%  797.54ms
        99%  957.54ms
        40000 requests in 1.48m, 15.34MB read
        Requests/sec:    449.44
        Transfer/sec:    176.52KB
        ------------------------------
        stop time: 51.879895
        stop time: 51.887417
        stop time: 52.345041
        stop time: 53.092394
        stop time: 53.086945
        stop time: 53.214559
        stop time: 53.202444
        stop time: 53.078511
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > command terminated with exit code 1
    [exp] Times: [51.879895, 51.887417, 52.345041, 53.092394, 53.086945, 53.214559, 53.202444, 53.078511]
    [exp] Throughput: 758.6764023373435
[test.py] Finished running 1th optmization experiment: groundtruth->1606.0858850311424, slowdown->758.6764023373435, predicted->1637.8875660410336, err->1.980073500818699
[test.py] Running 2th optmization experiment
[test.py] Running 2th groundtruth exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.001256', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   336.82ms  112.52ms   1.27s    88.59%
        Req/Sec   189.33     60.19   330.00     71.18%
        Latency Distribution
        50%  307.39ms
        75%  356.63ms
        90%  446.23ms
        99%  844.99ms
        4347 requests in 3.02s, 1.67MB read
        Requests/sec:   1438.70
        Transfer/sec:    564.33KB
        [run.sh] Speed is 1438.70, duration is 41
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d41s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        Running 41s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   317.37ms   84.12ms   1.30s    89.17%
        Req/Sec   201.96     45.91   373.00     70.02%
        Latency Distribution
        50%  302.06ms
        75%  336.72ms
        90%  383.14ms
        99%  629.25ms
        40000 requests in 41.00s, 15.36MB read
        Requests/sec:    975.61
        Transfer/sec:    383.58KB
        ------------------------------
        stop time: 24.451566
        stop time: 24.788134
        stop time: 24.911765
        stop time: 24.930114
        stop time: 24.947101
        stop time: 25.064919
        stop time: 25.069085
        stop time: 24.912171
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > error: metrics not available yet
    [exp] Times: [24.451566, 24.788134, 24.911765, 24.930114, 24.947101, 25.064919, 25.069085, 24.912171]
    [exp] Throughput: 1607.4355548318745
[test.py] Running 2th slowdown exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.00628834', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '629.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '62899990', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '629.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '62899990', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   632.65ms  115.27ms   1.09s    73.04%
        Req/Sec    98.16     45.26   200.00     64.65%
        Latency Distribution
        50%  623.76ms
        75%  695.22ms
        90%  760.66ms
        99%  978.59ms
        2185 requests in 3.02s, 854.57KB read
        Requests/sec:    722.76
        Transfer/sec:    282.68KB
        [run.sh] Speed is 722.76, duration is 83
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d83s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-6c498774b5-zx8ll        501m         50Mi
        service1-6cb89bcbc8-pz9zn        51m          17Mi
        service2-6f7c796b99-gltwr        181m         13Mi
        ubuntu-client-76886f6bbd-8wx6m   4m           12Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   637.47ms   95.91ms   1.57s    70.40%
        Req/Sec   100.47     41.16   450.00     71.14%
        Latency Distribution
        50%  623.91ms
        75%  685.72ms
        90%  752.94ms
        99%  905.61ms
        40000 requests in 1.38m, 15.38MB read
        Requests/sec:    481.93
        Transfer/sec:    189.70KB
        ------------------------------
        stop time: 49.608261
        stop time: 49.969502
        stop time: 49.868340
        stop time: 50.299639
        stop time: 50.204063
        stop time: 50.305223
        stop time: 50.285588
        stop time: 50.379884
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [49.608261, 49.969502, 49.86834, 50.299639, 50.204063, 50.305223, 50.285588, 50.379884]
    [exp] Throughput: 798.1632268741558
[test.py] Finished running 2th optmization experiment: groundtruth->1607.4355548318745, slowdown->798.1632268741558, predicted->1602.990378551187, err->-0.27653838235228356
[test.py] Running 3th optmization experiment
[test.py] Running 3th groundtruth exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.001884', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] service0-7564bb8985-dh4hq cannot connect to service1
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   346.51ms  134.38ms   1.13s    85.12%
        Req/Sec   181.31     68.55   380.00     69.79%
        Latency Distribution
        50%  303.88ms
        75%  368.67ms
        90%  512.28ms
        99%  848.34ms
        4261 requests in 3.02s, 1.64MB read
        Requests/sec:   1409.45
        Transfer/sec:    554.12KB
        [run.sh] Speed is 1409.45, duration is 42
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d42s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                        CPU(cores)   MEMORY(bytes)
        service0-7564bb8985-dh4hq   15m          41Mi
        Running 42s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   316.73ms   78.47ms   1.18s    86.76%
        Req/Sec   202.12     48.50   590.00     72.04%
        Latency Distribution
        50%  303.03ms
        75%  337.68ms
        90%  388.08ms
        99%  619.69ms
        40000 requests in 42.00s, 15.37MB read
        Requests/sec:    952.38
        Transfer/sec:    374.68KB
        ------------------------------
        stop time: 24.101636
        stop time: 24.744913
        stop time: 24.815433
        stop time: 25.105014
        stop time: 25.014051
        stop time: 25.057991
        stop time: 25.044257
        stop time: 25.049277
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > command terminated with exit code 1
    [exp] Times: [24.101636, 24.744913, 24.815433, 25.105014, 25.014051, 25.057991, 25.044257, 25.049277]
    [exp] Throughput: 1608.5852446526458
[test.py] Running 3th slowdown exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.00628834', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '550.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '55049990', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '550.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '55049990', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] service0-6987558776-4s9gq cannot connect to service1
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   594.25ms  120.57ms   1.01s    76.27%
        Req/Sec   101.86     57.20   333.00     65.62%
        Latency Distribution
        50%  591.57ms
        75%  655.22ms
        90%  728.64ms
        99%  909.26ms
        2322 requests in 3.03s, 0.89MB read
        Requests/sec:    767.58
        Transfer/sec:    302.03KB
        [run.sh] Speed is 767.58, duration is 78
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d78s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-6987558776-4s9gq        934m         49Mi
        service1-5b48bbff67-6ngxg        269m         16Mi
        service2-6f7c796b99-cnrf8        337m         11Mi
        ubuntu-client-76886f6bbd-5fjt4   16m          13Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   594.49ms   93.80ms   1.38s    72.14%
        Req/Sec   108.00     40.06   363.00     68.96%
        Latency Distribution
        50%  585.78ms
        75%  630.18ms
        90%  705.37ms
        99%  847.70ms
        40000 requests in 1.30m, 15.35MB read
        Requests/sec:    512.82
        Transfer/sec:    201.54KB
        ------------------------------
        stop time: 46.063211
        stop time: 46.305521
        stop time: 46.520866
        stop time: 46.824748
        stop time: 46.908216
        stop time: 46.888789
        stop time: 47.007621
        stop time: 46.995131
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > command terminated with exit code 1
    [exp] Times: [46.063211, 46.305521, 46.520866, 46.824748, 46.908216, 46.888789, 47.007621, 46.995131]
    [exp] Throughput: 856.728025608179
[test.py] Finished running 3th optmization experiment: groundtruth->1608.5852446526458, slowdown->856.728025608179, predicted->1621.562705756355, err->0.8067624110596325
[test.py] Running 4th optmization experiment
[test.py] Running 4th groundtruth exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.002512', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] service0-7564bb8985-vzpts cannot connect to service1
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   334.62ms  107.30ms 950.39ms   89.16%
        Req/Sec   192.97     58.10   320.00     72.77%
        Latency Distribution
        50%  303.90ms
        75%  346.96ms
        90%  435.74ms
        99%  821.19ms
        4379 requests in 3.02s, 1.68MB read
        Requests/sec:   1450.23
        Transfer/sec:    569.89KB
        [run.sh] Speed is 1450.23, duration is 41
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d41s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-7564bb8985-vzpts        614m         45Mi
        service1-5b899d549d-fcfxs        346m         16Mi
        service2-6f7c796b99-fx6jz        165m         9Mi
        ubuntu-client-76886f6bbd-wf8lt   13m          0Mi
        Running 41s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   315.56ms   92.97ms   1.49s    89.16%
        Req/Sec   204.80     52.13   626.00     72.10%
        Latency Distribution
        50%  301.74ms
        75%  336.69ms
        90%  384.90ms
        99%  672.51ms
        40000 requests in 41.00s, 15.34MB read
        Requests/sec:    975.61
        Transfer/sec:    383.07KB
        ------------------------------
        stop time: 24.111151
        stop time: 24.201220
        stop time: 24.376780
        stop time: 24.905887
        stop time: 24.961781
        stop time: 25.089880
        stop time: 25.101137
        stop time: 25.022791
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > command terminated with exit code 1
    [exp] Times: [24.111151, 24.20122, 24.37678, 24.905887, 24.961781, 25.08988, 25.101137, 25.022791]
    [exp] Throughput: 1618.0360291824327
[test.py] Running 4th slowdown exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.00628834', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '472.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '47199990', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '472.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '47199990', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   561.40ms  137.18ms   1.31s    79.27%
        Req/Sec   107.36     47.02   250.00     69.70%
        Latency Distribution
        50%  545.58ms
        75%  620.55ms
        90%  709.73ms
        99%    1.06s
        2484 requests in 3.03s, 0.95MB read
        Requests/sec:    820.05
        Transfer/sec:    320.74KB
        [run.sh] Speed is 820.05, duration is 73
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d73s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-79d7668d8c-v6gcl        967m         50Mi
        service1-867b4f8b64-7rvq7        941m         17Mi
        service2-6f7c796b99-j6nhz        1505m        11Mi
        ubuntu-client-76886f6bbd-rz458   48m          13Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   557.02ms   88.36ms   1.53s    73.01%
        Req/Sec   115.00     39.59   484.00     70.56%
        Latency Distribution
        50%  546.54ms
        75%  596.94ms
        90%  658.89ms
        99%  794.32ms
        40000 requests in 1.22m, 15.34MB read
        Requests/sec:    547.94
        Transfer/sec:    215.23KB
        ------------------------------
        stop time: 43.247604
        stop time: 43.581262
        stop time: 43.595032
        stop time: 43.769283
        stop time: 43.963242
        stop time: 43.967839
        stop time: 44.058263
        stop time: 44.044148
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [43.247604, 43.581262, 43.595032, 43.769283, 43.963242, 43.967839, 44.058263, 44.044148]
    [exp] Throughput: 913.6939721321567
[test.py] Finished running 4th optmization experiment: groundtruth->1618.0360291824327, slowdown->913.6939721321567, predicted->1606.6428818919708, err->-0.704134338480626
[test.py] Running 5th optmization experiment
[test.py] Running 5th groundtruth exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.00314', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] service0-7564bb8985-nx2mn cannot connect to service1
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   337.93ms   91.62ms 903.00ms   82.48%
        Req/Sec   190.12     62.44   390.00     79.20%
        Latency Distribution
        50%  311.23ms
        75%  362.20ms
        90%  464.60ms
        99%  676.63ms
        4294 requests in 3.03s, 1.65MB read
        Requests/sec:   1419.19
        Transfer/sec:    556.95KB
        [run.sh] Speed is 1419.19, duration is 42
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d42s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-7564bb8985-nx2mn        144m         48Mi
        service1-5b899d549d-7tzd5        596m         17Mi
        service2-6f7c796b99-m2gc6        428m         10Mi
        ubuntu-client-76886f6bbd-gqk5j   17m          13Mi
        Running 42s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   317.57ms   78.67ms   1.25s    87.38%
        Req/Sec   202.03     50.56   620.00     72.81%
        Latency Distribution
        50%  304.50ms
        75%  339.35ms
        90%  387.05ms
        99%  595.54ms
        40000 requests in 42.00s, 15.34MB read
        Requests/sec:    952.38
        Transfer/sec:    374.07KB
        ------------------------------
        stop time: 24.521836
        stop time: 24.758812
        stop time: 24.895697
        stop time: 25.124845
        stop time: 25.074106
        stop time: 25.031775
        stop time: 24.988407
        stop time: 25.068114
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > command terminated with exit code 1
    [exp] Times: [24.521836, 24.758812, 24.895697, 25.124845, 25.074106, 25.031775, 24.988407, 25.068114]
    [exp] Throughput: 1604.3028042932265
[test.py] Running 5th slowdown exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.00628834', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '393.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '39349990', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '393.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '39349990', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   522.85ms  121.76ms   1.15s    83.39%
        Req/Sec   113.74     45.86   230.00     69.23%
        Latency Distribution
        50%  511.25ms
        75%  544.68ms
        90%  641.72ms
        99%  957.49ms
        2667 requests in 3.03s, 1.02MB read
        Requests/sec:    880.24
        Transfer/sec:    344.55KB
        [run.sh] Speed is 880.24, duration is 68
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d68s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-56868f9956-chmn5        1223m        50Mi
        service1-5999975ff-fs4s8         1002m        17Mi
        service2-6f7c796b99-kr2qg        1614m        12Mi
        ubuntu-client-76886f6bbd-rqtvg   50m          13Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   517.52ms   88.57ms   1.40s    74.87%
        Req/Sec   123.64     38.44   434.00     73.04%
        Latency Distribution
        50%  508.88ms
        75%  554.68ms
        90%  617.96ms
        99%  757.33ms
        40000 requests in 1.13m, 15.33MB read
        Requests/sec:    588.23
        Transfer/sec:    230.81KB
        ------------------------------
        stop time: 40.201434
        stop time: 40.219189
        stop time: 40.591621
        stop time: 40.490939
        stop time: 40.845291
        stop time: 40.894467
        stop time: 40.979306
        stop time: 41.049548
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [40.201434, 40.219189, 40.591621, 40.490939, 40.845291, 40.894467, 40.979306, 41.049548]
    [exp] Throughput: 983.792646392842
[test.py] Finished running 5th optmization experiment: groundtruth->1604.3028042932265, slowdown->983.792646392842, predicted->1605.3120175990355, err->0.06290665970964292
[test.py] Running 6th optmization experiment
[test.py] Running 6th groundtruth exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.003768', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] service0-7564bb8985-gfwxd cannot connect to service1
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   337.81ms   99.78ms   1.16s    85.38%
        Req/Sec   188.30     54.09   272.00     76.32%
        Latency Distribution
        50%  309.52ms
        75%  358.91ms
        90%  454.93ms
        99%  693.92ms
        4314 requests in 3.03s, 1.65MB read
        Requests/sec:   1424.15
        Transfer/sec:    557.01KB
        [run.sh] Speed is 1424.15, duration is 42
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d42s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-7564bb8985-gfwxd        1942m        48Mi
        service1-5b899d549d-qn6fw        1649m        16Mi
        service2-6f7c796b99-2pbdj        1619m        10Mi
        ubuntu-client-76886f6bbd-tgh6w   77m          12Mi
        Running 42s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   317.39ms   67.13ms   1.22s    83.61%
        Req/Sec   201.80     45.77   430.00     69.21%
        Latency Distribution
        50%  304.94ms
        75%  339.93ms
        90%  387.09ms
        99%  565.33ms
        40000 requests in 42.00s, 15.34MB read
        Requests/sec:    952.38
        Transfer/sec:    373.94KB
        ------------------------------
        stop time: 24.714457
        stop time: 24.781567
        stop time: 24.926636
        stop time: 24.926050
        stop time: 24.965193
        stop time: 24.989650
        stop time: 25.134283
        stop time: 25.097735
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > command terminated with exit code 1
    [exp] Times: [24.714457, 24.781567, 24.926636, 24.92605, 24.965193, 24.98965, 25.134283, 25.097735]
    [exp] Throughput: 1603.7240798534112
[test.py] Running 6th slowdown exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.00628834', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '315.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '31499990', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '315.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '31499990', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] service0-5446c5d6cc-fcf2q cannot connect to service1
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   488.76ms  155.81ms   1.32s    83.13%
        Req/Sec   122.83     58.00   282.00     66.09%
        Latency Distribution
        50%  468.56ms
        75%  531.71ms
        90%  638.35ms
        99%    1.02s
        2898 requests in 3.02s, 1.11MB read
        Requests/sec:    958.63
        Transfer/sec:    375.02KB
        [run.sh] Speed is 958.63, duration is 62
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d62s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   474.48ms   87.30ms   1.65s    77.40%
        Req/Sec   134.99     42.35   353.00     68.95%
        Latency Distribution
        50%  462.54ms
        75%  514.27ms
        90%  571.23ms
        99%  732.37ms
        40000 requests in 1.03m, 15.34MB read
        Requests/sec:    645.16
        Transfer/sec:    253.40KB
        ------------------------------
        stop time: 36.780861
        stop time: 36.999588
        stop time: 37.041749
        stop time: 37.444583
        stop time: 37.380856
        stop time: 37.517367
        stop time: 37.445271
        stop time: 37.634448
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > command terminated with exit code 1
        > error: metrics not available yet
    [exp] Times: [36.780861, 36.999588, 37.041749, 37.444583, 37.380856, 37.517367, 37.445271, 37.634448]
    [exp] Throughput: 1072.944381986601
[test.py] Finished running 6th optmization experiment: groundtruth->1603.7240798534112, slowdown->1072.944381986601, predicted->1620.8184157469439, err->1.0659150229318206
[test.py] Running 7th optmization experiment
[test.py] Running 7th groundtruth exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.004396', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] service0-7564bb8985-zqxjb cannot connect to service1
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   343.97ms  115.95ms   1.29s    91.76%
        Req/Sec   189.70     54.94   343.00     71.43%
        Latency Distribution
        50%  311.93ms
        75%  361.00ms
        90%  442.34ms
        99%  890.44ms
        4280 requests in 3.03s, 1.64MB read
        Requests/sec:   1412.08
        Transfer/sec:    553.09KB
        [run.sh] Speed is 1412.08, duration is 42
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d42s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                        CPU(cores)   MEMORY(bytes)
        service1-5b899d549d-6hb45   345m         16Mi
        service2-6f7c796b99-f42nr   292m         11Mi
        Running 42s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   321.99ms   96.24ms   1.62s    88.95%
        Req/Sec   199.86     45.28   414.00     71.04%
        Latency Distribution
        50%  303.49ms
        75%  344.11ms
        90%  404.14ms
        99%  643.42ms
        40000 requests in 42.00s, 15.34MB read
        Requests/sec:    952.38
        Transfer/sec:    373.98KB
        ------------------------------
        stop time: 24.982977
        stop time: 25.121109
        stop time: 25.015194
        stop time: 25.204257
        stop time: 25.206903
        stop time: 25.349330
        stop time: 25.365457
        stop time: 25.406861
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > command terminated with exit code 1
    [exp] Times: [24.982977, 25.121109, 25.015194, 25.204257, 25.206903, 25.34933, 25.365457, 25.406861]
    [exp] Throughput: 1586.891577338887
[test.py] Running 7th slowdown exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.00628834', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '236.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '23649990', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '236.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '23649990', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] service0-5c656f8544-dwtwq cannot connect to service1
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   460.01ms  188.06ms   1.40s    85.01%
        Req/Sec   135.38     48.35   290.00     73.04%
        Latency Distribution
        50%  398.18ms
        75%  512.95ms
        90%  657.42ms
        99%    1.21s
        3127 requests in 3.03s, 1.19MB read
        Requests/sec:   1030.63
        Transfer/sec:    402.47KB
        [run.sh] Speed is 1030.63, duration is 58
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d58s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-5c656f8544-dwtwq        1084m        48Mi
        service1-7d9bfc5879-xr86t        1198m        19Mi
        service2-6f7c796b99-8b7m6        1901m        15Mi
        ubuntu-client-76886f6bbd-pgqbx   51m          13Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   435.54ms   90.54ms   1.56s    78.13%
        Req/Sec   147.39     41.43   380.00     68.70%
        Latency Distribution
        50%  416.11ms
        75%  475.59ms
        90%  559.88ms
        99%  707.58ms
        40000 requests in 0.97m, 15.33MB read
        Requests/sec:    689.65
        Transfer/sec:    270.68KB
        ------------------------------
        stop time: 33.936320
        stop time: 33.935623
        stop time: 34.065709
        stop time: 34.311158
        stop time: 34.266200
        stop time: 34.375177
        stop time: 34.508169
        stop time: 34.420268
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > command terminated with exit code 1
    [exp] Times: [33.93632, 33.935623, 34.065709, 34.311158, 34.2662, 34.375177, 34.508169, 34.420268]
    [exp] Throughput: 1168.6568113058665
[test.py] Finished running 7th optmization experiment: groundtruth->1586.891577338887, slowdown->1168.6568113058665, predicted->1615.1417601847206, err->1.7802213616387963
[test.py] Running 8th optmization experiment
[test.py] Running 8th groundtruth exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.005024', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   353.59ms  159.36ms   1.11s    78.77%
        Req/Sec   172.83     52.28   292.00     68.33%
        Latency Distribution
        50%  301.10ms
        75%  415.44ms
        90%  566.70ms
        99%  878.52ms
        4135 requests in 3.02s, 1.58MB read
        Requests/sec:   1370.78
        Transfer/sec:    535.34KB
        [run.sh] Speed is 1370.78, duration is 43
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d43s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-7564bb8985-mpfs9        1857m        49Mi
        service1-5b899d549d-fx2j8        1365m        19Mi
        service2-6f7c796b99-czfrk        1620m        19Mi
        ubuntu-client-76886f6bbd-fkbcf   70m          14Mi
        Running 43s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   349.64ms  134.26ms   1.88s    80.39%
        Req/Sec   183.54     47.39   808.00     72.00%
        Latency Distribution
        50%  298.70ms
        75%  366.20ms
        90%  544.11ms
        99%  737.44ms
        40000 requests in 43.00s, 15.34MB read
        Requests/sec:    930.23
        Transfer/sec:    365.21KB
        ------------------------------
        stop time: 26.841889
        stop time: 27.424299
        stop time: 27.708174
        stop time: 27.447138
        stop time: 27.327753
        stop time: 27.467148
        stop time: 27.316416
        stop time: 27.539469
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [26.841889, 27.424299, 27.708174, 27.447138, 27.327753, 27.467148, 27.316416, 27.539469]
    [exp] Throughput: 1460.705075218871
[test.py] Running 8th slowdown exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.00628834', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '158.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '15799990', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '158.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '15799990', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   428.04ms  169.80ms   1.45s    79.03%
        Req/Sec   142.58     45.61   250.00     71.00%
        Latency Distribution
        50%  358.14ms
        75%  482.67ms
        90%  686.00ms
        99%    1.02s
        3298 requests in 3.03s, 1.26MB read
        Requests/sec:   1088.07
        Transfer/sec:    424.81KB
        [run.sh] Speed is 1088.07, duration is 55
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d55s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   438.38ms  147.89ms   1.40s    78.93%
        Req/Sec   146.17     42.86   585.00     68.54%
        Latency Distribution
        50%  379.94ms
        75%  463.33ms
        90%  686.70ms
        99%  855.38ms
        40000 requests in 0.92m, 15.36MB read
        Requests/sec:    727.27
        Transfer/sec:    286.00KB
        ------------------------------
        stop time: 34.312026
        stop time: 33.605360
        stop time: 34.290336
        stop time: 34.468147
        stop time: 34.511636
        stop time: 34.797555
        stop time: 34.827297
        stop time: 34.863034
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > error: metrics not available yet
    [exp] Times: [34.312026, 33.60536, 34.290336, 34.468147, 34.511636, 34.797555, 34.827297, 34.863034]
    [exp] Throughput: 1160.7855124072355
[test.py] Finished running 8th optmization experiment: groundtruth->1460.705075218871, slowdown->1160.7855124072355, predicted->1421.5790935221835, err->-2.6785682038398337
[test.py] Running 9th optmization experiment
[test.py] Running 9th groundtruth exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.005652', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '10', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] service0-7564bb8985-xhhfl cannot connect to service1
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   419.86ms  157.85ms   1.40s    83.35%
        Req/Sec   154.64     64.36   320.00     65.16%
        Latency Distribution
        50%  361.58ms
        75%  486.83ms
        90%  644.95ms
        99%  978.21ms
        3438 requests in 3.02s, 1.31MB read
        Requests/sec:   1138.14
        Transfer/sec:    442.43KB
        [run.sh] Speed is 1138.14, duration is 52
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d52s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   391.80ms  135.16ms   1.25s    77.56%
        Req/Sec   163.62     47.21   470.00     70.00%
        Latency Distribution
        50%  333.58ms
        75%  411.61ms
        90%  628.24ms
        99%  751.94ms
        40000 requests in 0.87m, 15.35MB read
        Requests/sec:    769.23
        Transfer/sec:    302.19KB
        ------------------------------
        stop time: 30.125670
        stop time: 30.658167
        stop time: 30.611585
        stop time: 30.993902
        stop time: 31.049290
        stop time: 31.015028
        stop time: 31.024721
        stop time: 31.051293
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
        > command terminated with exit code 1
        > error: metrics not available yet
    [exp] Times: [30.12567, 30.658167, 30.611585, 30.993902, 31.04929, 31.015028, 31.024721, 31.051293]
    [exp] Throughput: 1298.0182797967318
[test.py] Running 9th slowdown exp
    [exp] Running (pre_run: False) workload synthetic/dynamic-cache-http-sync request with the following configuration: {'CLIENT_CPU_QUOTA': '2', 'SLOWPOKE_POKER_BATCH_THRESHOLD': '20000000', 'PROCESSING_TIME_SERVICE0': '0.00095807', 'PROCESSING_TIME_SERVICE1': '0.00082651', 'PROCESSING_TIME_SERVICE2': '0.00628834', 'SLOWPOKE_DELAY_MICROS_SERVICE0': '79.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE0': '7949990', 'SLOWPOKE_DELAY_MICROS_SERVICE1': '79.5', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE1': '7949990', 'SLOWPOKE_DELAY_MICROS_SERVICE2': '0.0', 'SLOWPOKE_POKER_BATCH_THRESHOLD_SERVICE2': '10'}
    [exp] Executing cmd `bash run.sh synthetic dynamic-cache-http-sync 8 512 40000`:
        [run.sh] Running benchmark synthetic with request dynamic-cache-http-sync, thread 8, conn 512, duration 60
        [run.sh] Deleting all services
        configmap "config-service0" deleted
        deployment.apps "service0" deleted
        service "service0" deleted
        configmap "config-service1" deleted
        deployment.apps "service1" deleted
        service "service1" deleted
        configmap "config-service2" deleted
        deployment.apps "service2" deleted
        service "service2" deleted
        deployment.apps "ubuntu-client" deleted
        [run.sh] Waiting for all pods to be deleted
        [run.sh] Deploying all services
        configmap/config-service0 created
        deployment.apps/service0 created
        service/service0 created
        configmap/config-service1 created
        deployment.apps/service1 created
        service/service1 created
        configmap/config-service2 created
        deployment.apps/service2 created
        service/service2 created
        [run.sh] Client pod not found, deploying client
        deployment.apps/ubuntu-client created
        [run.sh] Waiting for all pods to be running
        [run.sh] All pods are running
        [run.sh] Checking heartbeat for all services
        [run.sh] All pods can connect to all services
        [run.sh] Running warmup test
        [run.sh] /wrk/wrk -t8 -c512 -d3s -L http://service0:80/endpoint1
        Running 3s test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   420.28ms  221.19ms   1.64s    78.02%
        Req/Sec   146.22     49.62   292.00     70.69%
        Latency Distribution
        50%  360.97ms
        75%  470.14ms
        90%  723.74ms
        99%    1.21s
        3409 requests in 3.02s, 1.30MB read
        Requests/sec:   1127.37
        Transfer/sec:    439.86KB
        [run.sh] Speed is 1127.37, duration is 53
        [run.sh] Fix the request number.
        [run.sh] Running the actual test
        [run.sh] /wrk/wrk --timeout 20s -t8 -c512 -d53s -L -s /wrk/fix_req_n.lua http://service0:80/endpoint1
        [run.sh] Checking the resource usage
        NAME                             CPU(cores)   MEMORY(bytes)
        service0-869ff5c8f4-9jncd        309m         37Mi
        service1-7b9df4c58f-kfn2q        439m         18Mi
        service2-6f7c796b99-ff5x8        661m         15Mi
        ubuntu-client-76886f6bbd-dbfqs   18m          0Mi
        Running 1m test @ http://service0:80/endpoint1
        8 threads and 512 connections
        Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency   438.65ms  154.19ms   1.64s    77.35%
        Req/Sec   145.93     43.82   353.00     65.45%
        Latency Distribution
        50%  377.22ms
        75%  465.12ms
        90%  705.63ms
        99%  846.47ms
        40000 requests in 0.88m, 15.36MB read
        Requests/sec:    754.71
        Transfer/sec:    296.70KB
        ------------------------------
        stop time: 34.124386
        stop time: 34.210286
        stop time: 34.250326
        stop time: 34.375851
        stop time: 34.586473
        stop time: 34.717699
        stop time: 34.830278
        stop time: 34.694316
        [run.sh] Test finished with status 0
        > STDERR
        > No resources found in default namespace.
    [exp] Times: [34.124386, 34.210286, 34.250326, 34.375851, 34.586473, 34.717699, 34.830278, 34.694316]
    [exp] Throughput: 1160.3047489659825
[test.py] Finished running 9th optmization experiment: groundtruth->1298.0182797967318, slowdown->1160.3047489659825, predicted->1278.2819124128025, err->-1.5204999568280335
[test.py] Baseline throughput:  1184.935690392597
[test.py] Groundtruth:  [1614.7084519506993, 1606.0858850311424, 1607.4355548318745, 1608.5852446526458, 1618.0360291824327, 1604.3028042932265, 1603.7240798534112, 1586.891577338887, 1460.705075218871, 1298.0182797967318]
[test.py] Slowdown:  [708.1652632173711, 758.6764023373435, 798.1632268741558, 856.728025608179, 913.6939721321567, 983.792646392842, 1072.944381986601, 1168.6568113058665, 1160.7855124072355, 1160.3047489659825]
[test.py] Predicted:  [1597.2979787931135, 1637.8875660410336, 1602.990378551187, 1621.562705756355, 1606.6428818919708, 1605.3120175990355, 1620.8184157469439, 1615.1417601847206, 1421.5790935221835, 1278.2819124128025]
[test.py] Error percentage:  [-1.0782425233826276, 1.980073500818699, -0.27653838235228356, 0.8067624110596325, -0.704134338480626, 0.06290665970964292, 1.0659150229318206, 1.7802213616387963, -2.6785682038398337, -1.5204999568280335]
[test.py] ++++++++++++++++++++++++++++++++ Summary: 
[test.py] Result for the experiment 0: 
    Baseline throughput: 1184.935690392597
    Groundtruth: [1614.7084519506993, 1606.0858850311424, 1607.4355548318745, 1608.5852446526458, 1618.0360291824327, 1604.3028042932265, 1603.7240798534112, 1586.891577338887, 1460.705075218871, 1298.0182797967318]
    Slowdown:    [708.1652632173711, 758.6764023373435, 798.1632268741558, 856.728025608179, 913.6939721321567, 983.792646392842, 1072.944381986601, 1168.6568113058665, 1160.7855124072355, 1160.3047489659825]
    Predicted:   [1597.2979787931135, 1637.8875660410336, 1602.990378551187, 1621.562705756355, 1606.6428818919708, 1605.3120175990355, 1620.8184157469439, 1615.1417601847206, 1421.5790935221835, 1278.2819124128025]
    Error Perc:  [-1.0782425233826276, 1.980073500818699, -0.27653838235228356, 0.8067624110596325, -0.704134338480626, 0.06290665970964292, 1.0659150229318206, 1.7802213616387963, -2.6785682038398337, -1.5204999568280335]
